{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kiet21042003/Human_Detection_YOLOv8/blob/main/YOLOv8_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Download dataset**"
      ],
      "metadata": {
        "id": "L4g-QFHa8uCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "HÆ°á»›ng dáº«n cháº¡y notebook: https://drive.google.com/file/d/15x6XKcM9E7DP58TfxP4Gv858ux5KnROg/view"
      ],
      "metadata": {
        "id": "d6Nu9WS7WhH-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "wZ0QoXlH9YXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# https://drive.google.com/file/d/1--0QuKMwj31K-CSvD8oq5fceFweiFPuN/view?usp=share_link\n",
        "!gdown https://drive.google.com/u/0/uc?id=1--0QuKMwj31K-CSvD8oq5fceFweiFPuN&export=download"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnzY2-c11sFc",
        "outputId": "91ace922-b48b-4dd4-b00d-312d82b7d191"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/u/0/uc?id=1--0QuKMwj31K-CSvD8oq5fceFweiFPuN\n",
            "To: /content/ultralytics/human_detection_dataset.zip\n",
            "100% 2.67G/2.67G [00:22<00:00, 120MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -q /content/human_detection_dataset.zip"
      ],
      "metadata": {
        "id": "JTtKuPAuTIqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# **2. Install YOLOv8**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ultralytics/ultralytics"
      ],
      "metadata": {
        "id": "TUFPge7f_1ms",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65980270-1ff9-425c-960d-26f47053003a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ultralytics'...\n",
            "remote: Enumerating objects: 17390, done.\u001b[K\n",
            "remote: Counting objects: 100% (302/302), done.\u001b[K\n",
            "remote: Compressing objects: 100% (207/207), done.\u001b[K\n",
            "remote: Total 17390 (delta 158), reused 185 (delta 95), pack-reused 17088\u001b[K\n",
            "Receiving objects: 100% (17390/17390), 9.27 MiB | 8.92 MiB/s, done.\n",
            "Resolving deltas: 100% (12103/12103), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2bd2081-3abe-4e71-c99f-f98aae6f2205"
      },
      "source": [
        "%cd ultralytics\n",
        "!pip install ultralytics\n",
        "import ultralytics\n",
        "\n",
        "ultralytics.checks()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Ultralytics YOLOv8.0.207 ðŸš€ Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete âœ… (2 CPUs, 12.7 GB RAM, 32.1/78.2 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/ultralytics\n",
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qmJnoy6DmPiQ",
        "outputId": "0eb0240e-3b8b-42df-b2ac-12e63435befd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ultralytics\n",
            "Obtaining file:///content/ultralytics\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31mÃ—\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31mâ”‚\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31mÃ—\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31mâ•°â”€>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Download pretrained model**"
      ],
      "metadata": {
        "id": "0qrJz-qJNCE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EV6G_tNcNDw1",
        "outputId": "7927ce08-78f8-4fcd-a220-702e730dc65e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-08 12:07:30--  https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt\n",
            "Resolving github.com (github.com)... 140.82.121.4\n",
            "Connecting to github.com (github.com)|140.82.121.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/521807533/404b29b7-e374-406c-ab38-7d0796e5b627?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231108%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231108T120730Z&X-Amz-Expires=300&X-Amz-Signature=a620a1245a260a925d34094fca899e5f6535eb86f692d77279c58c38306a5c8f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=521807533&response-content-disposition=attachment%3B%20filename%3Dyolov8s.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-08 12:07:30--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/521807533/404b29b7-e374-406c-ab38-7d0796e5b627?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231108%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231108T120730Z&X-Amz-Expires=300&X-Amz-Signature=a620a1245a260a925d34094fca899e5f6535eb86f692d77279c58c38306a5c8f&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=521807533&response-content-disposition=attachment%3B%20filename%3Dyolov8s.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22573363 (22M) [application/octet-stream]\n",
            "Saving to: â€˜yolov8s.ptâ€™\n",
            "\n",
            "yolov8s.pt          100%[===================>]  21.53M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-11-08 12:07:30 (222 MB/s) - â€˜yolov8s.ptâ€™ saved [22573363/22573363]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **4. Training**"
      ],
      "metadata": {
        "id": "l1wDw2Se_2-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo train model=yolov8s.pt data=/content/ultralytics/ultralytics/human_detection_dataset/data.yaml epochs=20 imgsz=640"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsnkpIkq_17A",
        "outputId": "735aca78-69b3-4401-a8e5-5849f9203f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8s.pt to 'yolov8s.pt'...\n",
            "100% 21.5M/21.5M [00:00<00:00, 192MB/s]\n",
            "Ultralytics YOLOv8.0.207 ðŸš€ Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8s.pt, data=/content/ultralytics/ultralytics/human_detection_dataset/data.yaml, epochs=20, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train5, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, stream_buffer=False, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=/content/ultralytics/runs/detect/train5\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf'...\n",
            "100% 755k/755k [00:00<00:00, 18.4MB/s]\n",
            "2023-11-08 12:12:14.202397: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-08 12:12:14.202449: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-08 12:12:14.202494: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n",
            "  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  2                  -1  1     29056  ultralytics.nn.modules.block.C2f             [64, 64, 1, True]             \n",
            "  3                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  4                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  5                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  6                  -1  2    788480  ultralytics.nn.modules.block.C2f             [256, 256, 2, True]           \n",
            "  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n",
            "  8                  -1  1   1838080  ultralytics.nn.modules.block.C2f             [512, 512, 1, True]           \n",
            "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    591360  ultralytics.nn.modules.block.C2f             [768, 256, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 16                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 19                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1   1969152  ultralytics.nn.modules.block.C2f             [768, 512, 1]                 \n",
            " 22        [15, 18, 21]  1   2116435  ultralytics.nn.modules.head.Detect           [1, [128, 256, 512]]          \n",
            "Model summary: 225 layers, 11135987 parameters, 11135971 gradients, 28.6 GFLOPs\n",
            "\n",
            "Transferred 349/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /content/ultralytics/runs/detect/train5', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "Downloading https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt to 'yolov8n.pt'...\n",
            "100% 6.23M/6.23M [00:00<00:00, 117MB/s]\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/ultralytics/ultralytics/human_detection_dataset/train/labels... 2220 images, 0 backgrounds, 0 corrupt: 100% 2220/2220 [00:02<00:00, 825.23it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/ultralytics/ultralytics/human_detection_dataset/train/labels.cache\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/ultralytics/ultralytics/human_detection_dataset/val/labels... 1642 images, 0 backgrounds, 0 corrupt: 100% 1642/1642 [00:01<00:00, 935.42it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/ultralytics/ultralytics/human_detection_dataset/val/labels.cache\n",
            "Plotting labels to /content/ultralytics/runs/detect/train5/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/ultralytics/runs/detect/train5\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       1/20      4.01G       1.06      0.895      1.061        152        640: 100% 139/139 [02:00<00:00,  1.16it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:59<00:00,  1.14s/it]\n",
            "                   all       1642      13171      0.684      0.618      0.659      0.402\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       2/20      4.09G      1.097     0.8142      1.074        179        640: 100% 139/139 [01:53<00:00,  1.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:58<00:00,  1.12s/it]\n",
            "                   all       1642      13171      0.713       0.54      0.656      0.397\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       3/20      4.14G      1.126     0.8251       1.09        168        640: 100% 139/139 [01:51<00:00,  1.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:59<00:00,  1.15s/it]\n",
            "                   all       1642      13171      0.705       0.54      0.629      0.363\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       4/20       4.1G      1.089     0.7789      1.071        170        640: 100% 139/139 [01:52<00:00,  1.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:58<00:00,  1.13s/it]\n",
            "                   all       1642      13171       0.73      0.652      0.731      0.465\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       5/20      4.09G      1.067     0.7593      1.064        172        640: 100% 139/139 [01:51<00:00,  1.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:59<00:00,  1.15s/it]\n",
            "                   all       1642      13171      0.583      0.641      0.638      0.404\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       6/20      4.09G      1.038     0.7304      1.047        208        640: 100% 139/139 [01:51<00:00,  1.25it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:58<00:00,  1.12s/it]\n",
            "                   all       1642      13171       0.81      0.668      0.763      0.471\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       7/20      4.11G      1.006     0.7014      1.037        208        640: 100% 139/139 [01:54<00:00,  1.22it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:57<00:00,  1.10s/it]\n",
            "                   all       1642      13171      0.778       0.49      0.637      0.403\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       8/20      4.09G     0.9867     0.6875      1.028        171        640: 100% 139/139 [01:53<00:00,  1.23it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:57<00:00,  1.11s/it]\n",
            "                   all       1642      13171      0.819      0.648      0.762      0.487\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "       9/20      4.07G     0.9586      0.662      1.017        162        640: 100% 139/139 [01:52<00:00,  1.24it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:59<00:00,  1.14s/it]\n",
            "                   all       1642      13171      0.829      0.691      0.783      0.507\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      10/20      4.07G     0.9345     0.6419      1.005        155        640: 100% 139/139 [01:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:59<00:00,  1.15s/it]\n",
            "                   all       1642      13171      0.718      0.623      0.685      0.436\n",
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      11/20      4.03G     0.9223     0.6488      0.994         82        640: 100% 139/139 [01:50<00:00,  1.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:58<00:00,  1.12s/it]\n",
            "                   all       1642      13171       0.81      0.625      0.744      0.475\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      12/20      4.05G     0.9101     0.6269     0.9881         88        640: 100% 139/139 [01:46<00:00,  1.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:59<00:00,  1.14s/it]\n",
            "                   all       1642      13171      0.792      0.614      0.731      0.474\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      13/20      4.05G     0.8867     0.6081     0.9814        115        640: 100% 139/139 [01:47<00:00,  1.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:59<00:00,  1.14s/it]\n",
            "                   all       1642      13171      0.842      0.616      0.769      0.494\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      14/20      4.02G     0.8669     0.5929     0.9687         90        640: 100% 139/139 [01:46<00:00,  1.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:59<00:00,  1.14s/it]\n",
            "                   all       1642      13171      0.798      0.638      0.736      0.483\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      15/20      4.01G     0.8451     0.5756     0.9609        106        640: 100% 139/139 [01:45<00:00,  1.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:59<00:00,  1.14s/it]\n",
            "                   all       1642      13171      0.803      0.634       0.75      0.497\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      16/20      4.05G      0.829     0.5562     0.9535         97        640: 100% 139/139 [01:46<00:00,  1.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:56<00:00,  1.09s/it]\n",
            "                   all       1642      13171      0.789      0.649      0.757      0.507\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      17/20      4.02G      0.805     0.5417     0.9468        100        640: 100% 139/139 [01:44<00:00,  1.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:58<00:00,  1.13s/it]\n",
            "                   all       1642      13171      0.776       0.65      0.721      0.478\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      18/20      4.04G     0.7832     0.5278      0.938         89        640: 100% 139/139 [01:45<00:00,  1.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:58<00:00,  1.12s/it]\n",
            "                   all       1642      13171      0.819      0.654      0.756      0.503\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      19/20      4.02G     0.7671     0.5146     0.9315         92        640: 100% 139/139 [01:45<00:00,  1.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:58<00:00,  1.12s/it]\n",
            "                   all       1642      13171      0.773      0.665      0.765      0.517\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
            "      20/20      4.03G     0.7519     0.4973     0.9241         90        640: 100% 139/139 [01:44<00:00,  1.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [00:57<00:00,  1.10s/it]\n",
            "                   all       1642      13171      0.826      0.662       0.77      0.519\n",
            "\n",
            "20 epochs completed in 0.949 hours.\n",
            "Optimizer stripped from /content/ultralytics/runs/detect/train5/weights/last.pt, 22.5MB\n",
            "Optimizer stripped from /content/ultralytics/runs/detect/train5/weights/best.pt, 22.5MB\n",
            "\n",
            "Validating /content/ultralytics/runs/detect/train5/weights/best.pt...\n",
            "Ultralytics YOLOv8.0.207 ðŸš€ Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 52/52 [01:04<00:00,  1.23s/it]\n",
            "                   all       1642      13171      0.824      0.663       0.77      0.519\n",
            "Speed: 0.2ms preprocess, 3.0ms inference, 0.0ms loss, 2.3ms postprocess per image\n",
            "Results saved to \u001b[1m/content/ultralytics/runs/detect/train5\u001b[0m\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/train\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **5. Validating**"
      ],
      "metadata": {
        "id": "82dyG2Sm_7QO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo val model=/content/ultralytics/runs/detect/train5/weights/best.pt data=../human_detection_dataset/data.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta-rqI-BARxJ",
        "outputId": "3565df28-6924-436c-ebab-676ad63efd6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.0.207 ðŸš€ Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/human_detection_dataset/val/labels... 1642 images, 0 backgrounds, 0 corrupt: 100% 1642/1642 [00:04<00:00, 367.55it/s]\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/human_detection_dataset/val/labels.cache\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% 103/103 [01:09<00:00,  1.48it/s]\n",
            "                   all       1642      13171      0.823      0.664       0.77       0.52\n",
            "Speed: 0.3ms preprocess, 5.6ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/ultralytics/runs/detect/val\u001b[0m\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/val\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **6. Predict**"
      ],
      "metadata": {
        "id": "TFbtVhU-QbfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "filename = next(iter(uploaded))\n",
        "\n",
        "print(f\"Uploaded file: {filename}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "Jrz0FTceQegV",
        "outputId": "356e8eaf-67f7-4135-c4d0-077cc2675806"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7b284e8a-bf5c-4ab0-8bbc-dd493d5e4a32\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7b284e8a-bf5c-4ab0-8bbc-dd493d5e4a32\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 170113580_1342491209451456_4019948175732261693_n.jpg to 170113580_1342491209451456_4019948175732261693_n.jpg\n",
            "Uploaded file: 170113580_1342491209451456_4019948175732261693_n.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With uploaded image\n",
        "!yolo predict model=/content/ultralytics/runs/detect/train5/weights/best.pt \\\n",
        "    source='/content/ultralytics/170113580_1342491209451456_4019948175732261693_n.jpg'"
      ],
      "metadata": {
        "id": "ZqYj3lvRQelB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f543809-b6d4-439e-acd7-63ed0d34a9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.0.207 ðŸš€ Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\n",
            "image 1/1 /content/ultralytics/170113580_1342491209451456_4019948175732261693_n.jpg: 480x640 21 Humans, 78.9ms\n",
            "Speed: 4.2ms preprocess, 78.9ms inference, 98.2ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Results saved to \u001b[1m/content/ultralytics/runs/detect/predict7\u001b[0m\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With online image\n",
        "# https://c.files.bbci.co.uk/1260/production/_108240740_beatles-abbeyroad-index-reuters-applecorps.jpg\n",
        "!yolo predict model=/content/ultralytics/runs/detect/train5/weights/best.pt source='https://assets.weforum.org/article/image/XaHpf_z51huQS_JPHs-jkPhBp0dLlxFJwt-sPLpGJB0.jpg'"
      ],
      "metadata": {
        "id": "J0bc8O0VRMDf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2389268-f2fc-404d-9671-d2e6fae1def2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.0.207 ðŸš€ Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\n",
            "Downloading https://assets.weforum.org/article/image/XaHpf_z51huQS_JPHs-jkPhBp0dLlxFJwt-sPLpGJB0.jpg to 'XaHpf_z51huQS_JPHs-jkPhBp0dLlxFJwt-sPLpGJB0.jpg'...\n",
            "100% 621k/621k [00:00<00:00, 3.67MB/s]\n",
            "image 1/1 /content/ultralytics/XaHpf_z51huQS_JPHs-jkPhBp0dLlxFJwt-sPLpGJB0.jpg: 448x640 6 Humans, 71.9ms\n",
            "Speed: 3.9ms preprocess, 71.9ms inference, 80.6ms postprocess per image at shape (1, 3, 448, 640)\n",
            "Results saved to \u001b[1m/content/ultralytics/runs/detect/predict5\u001b[0m\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With youtube video\n",
        "!yolo predict model=/content/ultralytics/runs/detect/train5/weights/best.pt source='https://www.youtube.com/watch?v=f6YDKF0LVWw'"
      ],
      "metadata": {
        "id": "m13peynJRZOd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53ae2bf1-6611-4366-9bae-61f2afec75ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.0.207 ðŸš€ Python-3.10.12 torch-2.1.0+cu118 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\n",
            "1/1: https://www.youtube.com/watch?v=fr4ZG7h5IRg... Success âœ… (1447 frames of shape 1920x1080 at 23.98 FPS)\n",
            "\n",
            "0: 384x640 5 Humans, 2062.2ms\n",
            "0: 384x640 6 Humans, 46.5ms\n",
            "0: 384x640 4 Humans, 37.6ms\n",
            "0: 384x640 4 Humans, 30.4ms\n",
            "0: 384x640 4 Humans, 54.0ms\n",
            "0: 384x640 9 Humans, 22.5ms\n",
            "0: 384x640 7 Humans, 28.8ms\n",
            "0: 384x640 9 Humans, 40.2ms\n",
            "0: 384x640 10 Humans, 34.7ms\n",
            "0: 384x640 10 Humans, 35.4ms\n",
            "0: 384x640 10 Humans, 29.0ms\n",
            "0: 384x640 11 Humans, 11.7ms\n",
            "0: 384x640 10 Humans, 36.4ms\n",
            "0: 384x640 10 Humans, 21.0ms\n",
            "0: 384x640 8 Humans, 29.5ms\n",
            "0: 384x640 8 Humans, 34.7ms\n",
            "0: 384x640 7 Humans, 11.5ms\n",
            "0: 384x640 6 Humans, 19.4ms\n",
            "0: 384x640 9 Humans, 17.1ms\n",
            "0: 384x640 11 Humans, 10.8ms\n",
            "0: 384x640 9 Humans, 23.9ms\n",
            "0: 384x640 9 Humans, 10.9ms\n",
            "0: 384x640 7 Humans, 12.5ms\n",
            "0: 384x640 9 Humans, 11.8ms\n",
            "0: 384x640 10 Humans, 20.6ms\n",
            "0: 384x640 10 Humans, 12.9ms\n",
            "0: 384x640 8 Humans, 15.2ms\n",
            "0: 384x640 10 Humans, 12.9ms\n",
            "0: 384x640 9 Humans, 14.0ms\n",
            "0: 384x640 7 Humans, 10.9ms\n",
            "0: 384x640 7 Humans, 11.7ms\n",
            "0: 384x640 5 Humans, 10.8ms\n",
            "0: 384x640 4 Humans, 13.2ms\n",
            "0: 384x640 5 Humans, 18.4ms\n",
            "0: 384x640 5 Humans, 11.6ms\n",
            "0: 384x640 4 Humans, 12.8ms\n",
            "0: 384x640 3 Humans, 10.8ms\n",
            "0: 384x640 3 Humans, 11.5ms\n",
            "0: 384x640 1 Human, 13.8ms\n",
            "0: 384x640 2 Humans, 13.1ms\n",
            "0: 384x640 4 Humans, 38.5ms\n",
            "0: 384x640 4 Humans, 24.1ms\n",
            "0: 384x640 5 Humans, 18.5ms\n",
            "0: 384x640 8 Humans, 16.2ms\n",
            "0: 384x640 8 Humans, 10.9ms\n",
            "0: 384x640 9 Humans, 18.7ms\n",
            "0: 384x640 10 Humans, 11.5ms\n",
            "0: 384x640 11 Humans, 14.3ms\n",
            "0: 384x640 10 Humans, 11.3ms\n",
            "0: 384x640 9 Humans, 11.4ms\n",
            "0: 384x640 12 Humans, 14.9ms\n",
            "0: 384x640 9 Humans, 13.0ms\n",
            "0: 384x640 10 Humans, 14.2ms\n",
            "0: 384x640 12 Humans, 10.7ms\n",
            "0: 384x640 12 Humans, 11.5ms\n",
            "0: 384x640 11 Humans, 15.7ms\n",
            "0: 384x640 8 Humans, 11.5ms\n",
            "0: 384x640 11 Humans, 12.5ms\n",
            "0: 384x640 9 Humans, 11.4ms\n",
            "0: 384x640 10 Humans, 14.8ms\n",
            "0: 384x640 13 Humans, 13.1ms\n",
            "0: 384x640 11 Humans, 16.2ms\n",
            "0: 384x640 11 Humans, 11.5ms\n",
            "0: 384x640 12 Humans, 12.7ms\n",
            "0: 384x640 11 Humans, 11.3ms\n",
            "0: 384x640 9 Humans, 12.2ms\n",
            "0: 384x640 7 Humans, 12.5ms\n",
            "0: 384x640 8 Humans, 11.1ms\n",
            "0: 384x640 8 Humans, 11.9ms\n",
            "0: 384x640 9 Humans, 17.8ms\n",
            "0: 384x640 9 Humans, 16.6ms\n",
            "0: 384x640 9 Humans, 11.4ms\n",
            "0: 384x640 8 Humans, 11.1ms\n",
            "0: 384x640 8 Humans, 12.2ms\n",
            "0: 384x640 4 Humans, 10.7ms\n",
            "0: 384x640 7 Humans, 15.0ms\n",
            "0: 384x640 9 Humans, 11.5ms\n",
            "0: 384x640 9 Humans, 11.3ms\n",
            "0: 384x640 13 Humans, 11.7ms\n",
            "0: 384x640 11 Humans, 20.0ms\n",
            "0: 384x640 12 Humans, 11.9ms\n",
            "0: 384x640 10 Humans, 16.6ms\n",
            "0: 384x640 14 Humans, 11.8ms\n",
            "0: 384x640 8 Humans, 21.9ms\n",
            "0: 384x640 6 Humans, 20.1ms\n",
            "0: 384x640 5 Humans, 17.0ms\n",
            "0: 384x640 14 Humans, 15.0ms\n",
            "0: 384x640 10 Humans, 11.8ms\n",
            "0: 384x640 6 Humans, 11.3ms\n",
            "0: 384x640 4 Humans, 12.9ms\n",
            "0: 384x640 11 Humans, 11.2ms\n",
            "0: 384x640 9 Humans, 11.1ms\n",
            "0: 384x640 9 Humans, 18.8ms\n",
            "0: 384x640 7 Humans, 10.8ms\n",
            "0: 384x640 7 Humans, 11.9ms\n",
            "0: 384x640 3 Humans, 11.7ms\n",
            "0: 384x640 2 Humans, 11.7ms\n",
            "0: 384x640 2 Humans, 11.1ms\n",
            "0: 384x640 3 Humans, 15.0ms\n",
            "0: 384x640 1 Human, 16.1ms\n",
            "0: 384x640 2 Humans, 11.2ms\n",
            "0: 384x640 2 Humans, 12.1ms\n",
            "0: 384x640 5 Humans, 11.9ms\n",
            "0: 384x640 9 Humans, 11.8ms\n",
            "0: 384x640 9 Humans, 12.9ms\n",
            "0: 384x640 10 Humans, 12.9ms\n",
            "0: 384x640 10 Humans, 12.7ms\n",
            "0: 384x640 9 Humans, 12.3ms\n",
            "0: 384x640 9 Humans, 10.7ms\n",
            "0: 384x640 (no detections), 12.5ms\n",
            "0: 384x640 (no detections), 13.0ms\n",
            "0: 384x640 (no detections), 19.1ms\n",
            "0: 384x640 (no detections), 14.3ms\n",
            "0: 384x640 (no detections), 11.5ms\n",
            "0: 384x640 (no detections), 18.0ms\n",
            "0: 384x640 (no detections), 12.4ms\n",
            "0: 384x640 (no detections), 13.6ms\n",
            "0: 384x640 (no detections), 12.6ms\n",
            "0: 384x640 (no detections), 13.0ms\n",
            "0: 384x640 (no detections), 18.3ms\n",
            "0: 384x640 (no detections), 12.8ms\n",
            "0: 384x640 (no detections), 12.3ms\n",
            "0: 384x640 (no detections), 16.0ms\n",
            "0: 384x640 (no detections), 11.3ms\n",
            "0: 384x640 (no detections), 12.7ms\n",
            "0: 384x640 (no detections), 12.4ms\n",
            "0: 384x640 (no detections), 12.3ms\n",
            "0: 384x640 (no detections), 12.5ms\n",
            "0: 384x640 (no detections), 22.3ms\n",
            "0: 384x640 (no detections), 19.9ms\n",
            "0: 384x640 (no detections), 19.8ms\n",
            "0: 384x640 (no detections), 23.6ms\n",
            "0: 384x640 (no detections), 11.9ms\n",
            "0: 384x640 (no detections), 22.5ms\n",
            "0: 384x640 (no detections), 19.2ms\n",
            "0: 384x640 1 Human, 30.3ms\n",
            "0: 384x640 1 Human, 18.1ms\n",
            "0: 384x640 1 Human, 18.1ms\n",
            "0: 384x640 1 Human, 15.3ms\n",
            "0: 384x640 1 Human, 10.7ms\n",
            "Speed: 4.8ms preprocess, 31.0ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1m/content/ultralytics/runs/detect/predict6\u001b[0m\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **7. Export model (Optional)**"
      ],
      "metadata": {
        "id": "chOAUxomBTML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert weight file to other formats\n",
        "!yolo export model=/content/ultralytics/runs/detect/train5/weights/best.pt format=onnx"
      ],
      "metadata": {
        "id": "xXGWyhg2BVRi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6c971ce-ee43-4408-96e6-134b0e224113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.0.207 ðŸš€ Python-3.10.12 torch-2.1.0+cu118 CPU (Intel Xeon 2.00GHz)\n",
            "Model summary (fused): 168 layers, 11125971 parameters, 0 gradients, 28.4 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from '/content/ultralytics/runs/detect/train5/weights/best.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 5, 8400) (21.5 MB)\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['onnx>=1.12.0'] not found, attempting AutoUpdate...\n",
            "Collecting onnx>=1.12.0\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "     â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â” 15.7/15.7 MB 67.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (1.23.5)\n",
            "Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx>=1.12.0) (3.20.3)\n",
            "Installing collected packages: onnx\n",
            "Successfully installed onnx-1.15.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success âœ… 8.1s, installed 1 package: ['onnx>=1.12.0']\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m âš ï¸ \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.15.0 opset 17...\n",
            "\u001b[34m\u001b[1mONNX:\u001b[0m export success âœ… 9.4s, saved as '/content/ultralytics/runs/detect/train5/weights/best.onnx' (42.6 MB)\n",
            "\n",
            "Export complete (12.8s)\n",
            "Results saved to \u001b[1m/content/ultralytics/runs/detect/train5/weights\u001b[0m\n",
            "Predict:         yolo predict task=detect model=/content/ultralytics/runs/detect/train5/weights/best.onnx imgsz=640  \n",
            "Validate:        yolo val task=detect model=/content/ultralytics/runs/detect/train5/weights/best.onnx imgsz=640 data=/content/ultralytics/ultralytics/human_detection_dataset/data.yaml  \n",
            "Visualize:       https://netron.app\n",
            "ðŸ’¡ Learn more at https://docs.ultralytics.com/modes/export\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Káº¿t quáº£ visualize model: https://husteduvn-my.sharepoint.com/:i:/g/personal/kiet_pt214909_sis_hust_edu_vn/EZ7TM9rEUNROhrMJnWu2xgsBtWtJkoEohQPrBRW9sGGarA?e=cnSh0e"
      ],
      "metadata": {
        "id": "p2DSDS8zREaI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/ultralytics/runs/detect/train/weights/best.onnx' '/content/gdrive/MyDrive/Coordinate/aio_2023_ta/module1/yolov8_project/solution/weights'\n",
        "!cp '/content/ultralytics/runs/detect/train/weights/best.pt' '/content/gdrive/MyDrive/Coordinate/aio_2023_ta/module1/yolov8_project/solution/weights'"
      ],
      "metadata": {
        "id": "WrjwWoHcBwlw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}